{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model\n",
      "Loading classification model from base model's checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification, DistilBertModel\n",
    "print('Loading base model')\n",
    "base_model = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
    "print(\"Loading classification model from base model's checkpoint\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertTokenizerFast, AutoTokenizer\n",
    "name = \"distilbert/distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "\n",
    "\n",
    "# DataLoader(zip(list1, list2))\n",
    "dataset_name = \"stanfordnlp/imdb\"\n",
    "\n",
    "imdb_dataset = load_dataset(dataset_name)\n",
    "\n",
    "\n",
    "# Just take the first 50 tokens for speed/running on cpu\n",
    "def truncate(example):\n",
    "    return {\n",
    "        'text': \" \".join(example['text'].split()[:50]),\n",
    "        'label': example['label']\n",
    "    }\n",
    "\n",
    "# Take 128 random examples for train and 32 validation\n",
    "small_imdb_dataset = DatasetDict(\n",
    "    train=imdb_dataset['train'].shuffle(seed=1111).select(range(128)).map(truncate),\n",
    "    val=imdb_dataset['train'].shuffle(seed=1111).select(range(128, 160)).map(truncate),\n",
    ")\n",
    "\n",
    "# Prepare the dataset - this tokenizes the dataset in batches of 16 examples.\n",
    "small_tokenized_dataset = small_imdb_dataset.map(\n",
    "    lambda example: tokenizer(example['text'], padding=True, truncation=True), # https://huggingface.co/docs/transformers/pad_truncation\n",
    "    batched=True,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "small_tokenized_dataset = small_tokenized_dataset.remove_columns([\"text\"])\n",
    "small_tokenized_dataset = small_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "small_tokenized_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\"Probably Jackie Chan's best film in the 1980s, and the one that put him on the map. The scale of this self-directed police drama is evident from the opening and closing scenes, during which a squatters' village and shopping mall are demolished. There are, clearly, differences between the original Chinese\",\n",
       "  'A wonderful movie! Anyone growing up in an Italian family will definitely see themselves in these characters. A good family movie with sadness, humor, and very good acting from all. You will enjoy this movie!! We need more like it.',\n",
       "  'HORRENDOUS! Avoid like the plague. I would rate this in the top 10 worst movies ever. Special effects, acting, mood, sound, etc. appear to be done by day care students...wait, I have seen programs better than this. Opens like a soft porn show with a blurred nude female doing a',\n",
       "  'And I absolutely adore Isabelle Blais!!! She was so cute in this movie, and far different from her role in \"Quebec-Montreal\" where she was more like a man-eater. I think she should have been nominated for a Jutra. I mean, Syvlie Moreau was good, but Isabelle was far superior, IMO.',\n",
       "  'Must confess to having seen a few howlers in my time, but this one is up there with the worst of them. Plot troubling to follow. Sex and violence thrown in to disorient and distract from the really poorly put together film.<br /><br />I can only imagine that the cast',\n",
       "  \"I pity people calling kamal hassan 'ulaganaayakan' maybe for them ulagam is tollywood ! comeon guys..this movie is a thriller without thrill..<br /><br />come out of your ulagam and just watch some high class thrillers like The Usual Suspects or even The Silence of the Lambs.<br /><br />technically good but\",\n",
       "  'I remember my parents not understanding Saturday Night Live when I was 15. They also did not understand Rock n Roll and many other things. Now that I am approaching their age, I still remember, and find I understand many of the things my kids love. But this is pathetic.',\n",
       "  'This animated movie is a masterpiece! The narration, music, animation, and storyline where all remarkable. My girlfriend and I saw it again for a second time and we got more insight from it. We invited a couple friends to see Spirit with us and they really enjoyed it a lot.',\n",
       "  'I vaugely recall seeing this when I was 3 years old, then my parents accidentally taped over all but a few seconds of it with some other cartoon. Then I was about 8 or 9 years old when I rediscovered it and since I was then able to comprehend things',\n",
       "  'I, like many people, saw this film in the theatre when it first came out in \\'97. It was a below average film at best, defiantly not the \"masterpiece\" that all these \"Titanic\" fanboys like to make it out as. First off, DiCaprio is a terrible actor no matter which'],\n",
       " 'label': [1, 1, 0, 1, 0, 0, 0, 1, 1, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_imdb_dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([1, 1]),\n",
       " 'input_ids': tensor([[  101, 10109,  9662, 10185,   112,   188,  1436,  1273,  1107,  1103,\n",
       "           3011,   117,  1105,  1103,  1141,  1115,  1508,  1140,  1113,  1103,\n",
       "           4520,   119,  1109,  3418,  1104,  1142,  2191,   118,  2002,  2021,\n",
       "           3362,  1110, 10238,  1121,  1103,  2280,  1105,  5134,  4429,   117,\n",
       "           1219,  1134,   170,  4816,  6718, 18899,   112,  1491,  1105,  6001,\n",
       "           8796,  1132,  6515,   119,  1247,  1132,   117,  3817,   117,  5408,\n",
       "           1206,  1103,  1560,  1922,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,   138,  7310,  2523,   106, 15859,  2898,  1146,  1107,  1126,\n",
       "           2169,  1266,  1209,  5397,  1267,  2310,  1107,  1292,  2650,   119,\n",
       "            138,  1363,  1266,  2523,  1114, 12928,   117,  8594,   117,  1105,\n",
       "           1304,  1363,  3176,  1121,  1155,   119,  1192,  1209,  5548,  1142,\n",
       "           2523,   106,   106,  1284,  1444,  1167,  1176,  1122,   119,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_tokenized_dataset['train'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset - this tokenizes the dataset in batches of 16 examples.\n",
    "small_tokenized_dataset = small_imdb_dataset.map(\n",
    "    lambda example: tokenizer(example['text'], padding=True, truncation=True), # https://huggingface.co/docs/transformers/pad_truncation\n",
    "    batched=True,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "small_tokenized_dataset = small_tokenized_dataset.remove_columns([\"text\"])\n",
    "small_tokenized_dataset = small_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "small_tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(small_tokenized_dataset['train'], batch_size=16)\n",
    "eval_dataloader = DataLoader(small_tokenized_dataset['val'], batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params} all model parameters: {all_model_params} percentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainable model parameters: 65783042 all model parameters: 65783042 percentage of trainable model parameters: 100.00%'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_number_of_trainable_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank Number\n",
    "    lora_alpha=32, # Alpha (Scaling Factor)\n",
    "    lora_dropout=0.05, # Dropout Prob for Lora\n",
    "    target_modules=[\"q_lin\", \"k_lin\",\"v_lin\"], # Which layer to apply LoRA, usually only apply on MultiHead Attention Layer\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_CLS # Seqence to Classification Task\n",
    ")\n",
    "\n",
    "lora_config_1 = LoraConfig(\n",
    "    r=8, # Rank Number\n",
    "    lora_alpha=16, # Alpha (Scaling Factor)\n",
    "    lora_dropout=0.01, # Dropout Prob for Lora\n",
    "    target_modules=[\"q_lin\", \"k_lin\",\"v_lin\"], # Which layer to apply LoRA, usually only apply on MultiHead Attention Layer\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_CLS # Seqence to Classification Task\n",
    ")\n",
    "\n",
    "lora_config_2 = LoraConfig(\n",
    "    r=16, # Rank Number\n",
    "    lora_alpha=8, # Alpha (Scaling Factor)\n",
    "    lora_dropout=0.002, # Dropout Prob for Lora\n",
    "    target_modules=[\"q_lin\", \"k_lin\",\"v_lin\"], # Which layer to apply LoRA, usually only apply on MultiHead Attention Layer\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_CLS # Seqence to Classification Task\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainable model parameters: 1034498 all model parameters: 66817540 percentage of trainable model parameters: 1.55%'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_number_of_trainable_model_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teddy3090/miniconda3/envs/hw/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "arguments = TrainingArguments(\n",
    "    output_dir=\"./result\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=224,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # calculates the accuracy\n",
    "    return {\"accuracy\": np.mean(predictions == labels)}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    #model=model,\n",
    "    model=peft_model,\n",
    "    args=arguments,\n",
    "    train_dataset=small_tokenized_dataset['train'],\n",
    "    eval_dataset=small_tokenized_dataset['val'], # change to test when you do your final evaluation!\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 92])\n",
      "Labels shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    print(\"Input shape:\", input_ids.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 00:08, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.669915</td>\n",
       "      <td>0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.653657</td>\n",
       "      <td>0.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.656495</td>\n",
       "      <td>0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.579565</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.530979</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.489540</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.465196</td>\n",
       "      <td>0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.439610</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.429755</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teddy3090/miniconda3/envs/hw/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/teddy3090/miniconda3/envs/hw/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/teddy3090/miniconda3/envs/hw/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/teddy3090/miniconda3/envs/hw/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/teddy3090/miniconda3/envs/hw/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/teddy3090/miniconda3/envs/hw/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/teddy3090/miniconda3/envs/hw/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/teddy3090/miniconda3/envs/hw/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/teddy3090/miniconda3/envs/hw/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/teddy3090/miniconda3/envs/hw/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=0.5521339416503906, metrics={'train_runtime': 8.4197, 'train_samples_per_second': 152.024, 'train_steps_per_second': 9.501, 'total_flos': 32406528232320.0, 'train_loss': 0.5521339416503906, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teddy3090/miniconda3/envs/hw/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "test_str = \"I enjoyed the movie!\"\n",
    "\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(\"result/checkpoint-40\")\n",
    "model_inputs = tokenizer(test_str, return_tensors=\"pt\")\n",
    "prediction = torch.argmax(finetuned_model(**model_inputs).logits)\n",
    "print([\"NEGATIVE\", \"POSITIVE\"][prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluating the model is very easy\n",
    "\n",
    "results = trainer.evaluate()                           # just gets evaluation metrics\n",
    "results = trainer.predict(small_tokenized_dataset['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.03740241, -0.20690368],\n",
       "       [ 0.12224149, -0.24344756],\n",
       "       [-0.17776576, -0.03724482],\n",
       "       [-0.13465351, -0.02291623],\n",
       "       [-1.5709504 ,  1.0764621 ],\n",
       "       [-0.16820702, -0.05394708],\n",
       "       [ 0.48865572, -0.5074051 ],\n",
       "       [ 0.39148882, -0.5054668 ],\n",
       "       [-1.160519  ,  0.7769239 ],\n",
       "       [-1.251047  ,  0.8715321 ],\n",
       "       [-0.38490903,  0.13534473],\n",
       "       [ 0.91885054, -0.9023751 ],\n",
       "       [ 0.5671619 , -0.5953146 ],\n",
       "       [-0.37796575,  0.17148209],\n",
       "       [ 0.5872896 , -0.68405783],\n",
       "       [ 0.89205205, -0.874074  ],\n",
       "       [ 0.04627319, -0.20756498],\n",
       "       [-1.312896  ,  0.91442865],\n",
       "       [-0.41346422,  0.16097899],\n",
       "       [-2.0610247 ,  1.4779649 ],\n",
       "       [ 0.6141318 , -0.70883626],\n",
       "       [ 0.1030957 , -0.21508381],\n",
       "       [-0.17825243, -0.00685348],\n",
       "       [-0.55719244,  0.2561368 ],\n",
       "       [ 0.02980587, -0.17486508],\n",
       "       [-0.7932595 ,  0.46205375],\n",
       "       [-0.8218774 ,  0.4770876 ],\n",
       "       [-0.78745306,  0.4902932 ],\n",
       "       [-0.4641882 ,  0.29385716],\n",
       "       [ 0.4395487 , -0.56798244],\n",
       "       [-0.21454275, -0.03536177],\n",
       "       [-1.4018914 ,  0.9410846 ]], dtype=float32), label_ids=array([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 0, 1]), metrics={'test_loss': 0.4297546446323395, 'test_accuracy': 0.78125, 'test_runtime': 0.0253, 'test_samples_per_second': 1266.994, 'test_steps_per_second': 79.187})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
